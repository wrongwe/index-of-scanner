import os
import sys
import re
import csv
import time
import signal
import asyncio
import aiohttp
from urllib.parse import urlparse, urlunparse, urljoin, parse_qs
from collections import defaultdict

from bs4 import BeautifulSoup
from pybloom_live import ScalableBloomFilter
from fake_useragent import UserAgent
import logging

# é…ç½®ä¼˜åŒ–ï¼ˆèšç„¦æ•æ„Ÿæ–‡ä»¶æ£€æµ‹ï¼‰
CONFIG = {
    "max_depth": 3,  # é™åˆ¶é€’å½’æ·±åº¦
    "request_timeout": 25,
    "concurrency_range": (30, 200),
    "forbidden_ports": {22, 3306, 3389},
    "sensitive_ext": {
        'config', 'ini', 'env', 'zip', 'bak', 'key', 'conf', 'properties',
        'sql', 'db', 'dbf', 'pem', 'crt', 'jks', 'p12', 'audit'
    },
    "sensitive_paths": [
        re.compile(r'/(backup|archive)/', re.I),
        re.compile(r'\.(git|svn)/', re.I)
    ],
    "ignore_ext": {'png', 'jpg', 'jpeg', 'gif'}
}

# æ—¥å¿—ç³»ç»Ÿé…ç½®ï¼ˆä¸­æ–‡è¾“å‡ºï¼‰
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("scan.log", mode='a', encoding='utf-8'),
        logging.StreamHandler()
    ]
)


class ScannerCore:
    def __init__(self):
        self.dedup_filter = ScalableBloomFilter(
            initial_capacity=10000, error_rate=0.001)
        self.ua = UserAgent()
        self.stats = defaultdict(int)
        self.findings = defaultdict(list)
        self.concurrency_sem = asyncio.Semaphore(CONFIG["concurrency_range"][1])
        self._shutdown = False
        self.session = None
        signal.signal(signal.SIGINT, self._graceful_shutdown)

    def _graceful_shutdown(self, signum, frame):
        logging.info("æ¥æ”¶åˆ°ç»ˆæ­¢ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢æ‰«æ...")
        self._shutdown = True

    async def _process_url(self, raw_url):
        """URLæ ‡å‡†åŒ–å¤„ç†"""
        url = raw_url.strip().lower()
        if not url.startswith(('http://', 'https://')):
            url = f'http://{url}'

        parsed = urlparse(url)
        if parsed.port in CONFIG["forbidden_ports"]:
            raise ValueError(f"ç¦æ­¢è®¿é—®é«˜å±ç«¯å£: {parsed.geturl()}")

        # æ¸…ç†è·Ÿè¸ªå‚æ•°
        query = parse_qs(parsed.query)
        clean_query = '&'.join(
            f"{k}={v[0]}" for k, v in query.items()
            if not k.startswith('utm_')
        )
        return urlunparse((
            parsed.scheme, parsed.netloc, parsed.path.rstrip('/'),
            parsed.params, clean_query, parsed.fragment
        ))

    def _detect_sensitive(self, url):
        """æ•æ„Ÿæ–‡ä»¶æ£€æµ‹é€»è¾‘"""
        parsed = urlparse(url)
        path = parsed.path.lower()

        # æ‰©å±•åæ£€æµ‹
        if (ext := path.split('.')[-1]) in CONFIG["sensitive_ext"]:
            return (True, f"æ•æ„Ÿæ‰©å±•å: {ext}")

        # æ­£åˆ™è·¯å¾„åŒ¹é…
        for pattern in CONFIG["sensitive_paths"]:
            if pattern.search(path):
                return (True, f"è·¯å¾„åŒ¹é…: {pattern.pattern}")

        # å¤šå±‚å‹ç¼©æ£€æµ‹
        parts = path.split('.')
        if len(parts) > 2 and any(p in CONFIG["sensitive_ext"] for p in parts[-2:]):
            return (True, f"å¤åˆå‹ç¼©æ–‡ä»¶: {'+'.join(parts[-2:])}")

        return (False, None)

    async def scan(self, url, depth=0):
        if self._shutdown or depth > CONFIG["max_depth"]:
            return

        try:
            processed_url = await self._process_url(url)
            if not processed_url:
                return

            # æ•æ„Ÿæ–‡ä»¶å¿«é€Ÿæ£€æµ‹
            is_sensitive, reason = self._detect_sensitive(processed_url)
            if is_sensitive:
                self.findings["SENSITIVE_FILES"].append({
                    "url": processed_url,
                    "reason": reason
                })
                return

            async with self.concurrency_sem:
                async with self.session.get(
                        processed_url,
                        allow_redirects=False,
                        timeout=aiohttp.ClientTimeout(total=CONFIG["request_timeout"])
                ) as resp:
                    self.stats['success_requests'] += 1

                    # ä»…è§£æé¦–å±‚HTMLé“¾æ¥
                    if depth == 0 and 'text/html' in resp.headers.get('Content-Type', ''):
                        content = await resp.text()
                        soup = BeautifulSoup(content, 'lxml')
                        for tag in soup.select('a[href]'):
                            new_url = urljoin(processed_url, tag['href'])
                            await self._schedule_task(new_url, depth + 1)

        except Exception as e:
            self.stats['failed_requests'] += 1
            logging.debug(f"è¯·æ±‚å¼‚å¸¸: {str(e)}")

    async def _schedule_task(self, url, depth):
        """ä»»åŠ¡è°ƒåº¦ç®¡ç†"""
        if (not self._shutdown and
                depth <= CONFIG["max_depth"] and
                url not in self.dedup_filter):
            self.dedup_filter.add(url)
            task = asyncio.create_task(self.scan(url, depth))
            await asyncio.wait_for(task, timeout=CONFIG["request_timeout"])

    async def _log_progress(self):
        """å®æ—¶è¿›åº¦æ˜¾ç¤º"""
        while not self._shutdown:
            elapsed = time.time() - self.stats['start_time']
            sys.stdout.write(
                f"\ræ‰«æè¿›åº¦ | æˆåŠŸè¯·æ±‚: {self.stats['success_requests']}æ¬¡ | "
                f"æ•æ„Ÿæ–‡ä»¶: {len(self.findings['SENSITIVE_FILES'])}ä¸ª | "
                f"è¿è¡Œæ—¶é•¿: {elapsed:.1f}ç§’"
            )
            sys.stdout.flush()
            await asyncio.sleep(0.8)

    async def generate_reports(self):
        """ç”Ÿæˆä¸­æ–‡æŠ¥å‘Š"""
        filename = f"å®‰å…¨æ‰«ææŠ¥å‘Š_{time.strftime('%Y%m%d_%H%M')}.csv"
        try:
            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow(["é£é™©ç­‰çº§", "URLåœ°å€", "æ£€æµ‹ä¾æ®"])

                for item in self.findings["SENSITIVE_FILES"]:
                    writer.writerow(["é«˜å±", item["url"], item["reason"]])

            logging.info(f"æŠ¥å‘Šæ–‡ä»¶å·²ç”Ÿæˆ: {os.path.abspath(filename)}")
            return filename
        except Exception as e:
            logging.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
            return None

    async def run(self, targets):
        self.session = aiohttp.ClientSession(
            headers={"User-Agent": self.ua.random}
        )
        self.stats['start_time'] = time.time()
        progress_task = asyncio.create_task(self._log_progress())

        try:
            tasks = [self.scan(url) for url in {t.strip() for t in targets if t.strip()}]
            await asyncio.gather(*tasks)
        finally:
            progress_task.cancel()
            await self.session.close()

        report_path = await self.generate_reports()
        print(f"\næ‰«æå®Œæˆï¼ŒæŠ¥å‘Šè·¯å¾„: {os.path.abspath(report_path)}")


# ä¿®æ”¹ä¸»ç¨‹åºå…¥å£é€»è¾‘ï¼ˆæ–°å¢äº¤äº’æ¨¡å¼ï¼‰
if __name__ == "__main__":
    targets = []

    try:
        # äº¤äº’å¼è¾“å…¥å¤„ç†
        if len(sys.argv) == 1:
            print("\nğŸ›¡ï¸ index of/æ¼æ´æ‰«æå™¨äº¤äº’æ¨¡å¼")
            print("----------------------------------------")
            filename = input("è¯·è¾“å…¥ç›®æ ‡æ–‡ä»¶è·¯å¾„: ").strip(' "\'')

            if not os.path.exists(filename):
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}")

            with open(filename) as f:
                targets = [line.strip() for line in f if line.strip()]

        elif len(sys.argv) == 2:
            with open(sys.argv[1]) as f:
                targets = [line.strip() for line in f if line.strip()]

        else:
            print("é”™è¯¯: å‚æ•°è¿‡å¤š")
            print("ç”¨æ³•: python scanner.py [ç›®æ ‡æ–‡ä»¶]")
            sys.exit(1)

        # å¯åŠ¨æ‰«æå¼•æ“
        print("\nğŸ” æ­£åœ¨åˆå§‹åŒ–æ‰«æå¼•æ“...")
        scanner = ScannerCore()
        asyncio.run(scanner.run(targets))

    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸»åŠ¨ç»ˆæ­¢æ‰«æè¿›ç¨‹")
    except FileNotFoundError as e:
        print(f"\nâ€¼ï¸ æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
        print("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
    except Exception as e:
        print(f"\nâ€¼ï¸ ç³»ç»Ÿå¼‚å¸¸: {str(e)}")